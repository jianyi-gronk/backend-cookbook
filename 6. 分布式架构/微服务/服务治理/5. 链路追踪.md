## 1. Opentelemetry，Prometheus，VictoriaMetrics 和 Grafana

- OpenTelemetry、Prometheus/VictoriaMetrics 和 Grafana 组成了一个典型的监控和可视化工作流
  1. Opentelemetry 用于注入在微服务中，从而自动检测、收集数据到将数据导出到监控系统（ 如 Prometheus 或 VictoriaMetrics ）
  2. Prometheus/VictoriaMetrics 用于 接收、存储并查询 监控数据
  3. Grafana 用 Prometheus/VictoriaMetrics 存储的数据构建可视化平台

## 2. OpenTelemetry

- OpenTelemetry 是一个观测性框架，它提供了一组工具来自动检测、收集遥测数据（ 如 追踪、度量 和 日志 ）和导出应用程序及其环境的遥测数据

#### 2.1 自动检测

- 在 OpenTelemetry 中，自动检测是指框架能够自动识别和监控应用程序中使用的库和框架，例如 HTTP 请求、数据库调用和其他资源的操作
- 通过自动检测，开发者不需要手动编写代码来捕获遥测数据，因为 OpenTelemetry 提供了一个范围广泛的检测器（ instrumentation ），这些检测器预先配置好了对常见库和框架的监控能力（ 如 HTTP、gRPC、Express 等 ）
- 自动检测通常通过在应用程序中添加 OpenTelemetry 的库来实现。例如，在 node 中这通常涉及到安装特定的 OpenTelemetry 包（ @opentelemetry/auto-instrumentations-node ），并在应用程序启动时初始化它们

#### 2.2 收集数据

- 数据收集是自动检测过程中自然发生的，一旦检测器被加载到应用程序中，它会收集各种遥测数据：
  - 追踪：记录应用流程中的操作
  - 度量：跟踪应用的性能指标
  - 日志：则收集应用运行期间的重要事件信息等
- OpenTelemetry 还允许通过 API 手动捕获遥测数据，这为开发者提供了更灵活的遥测策略，可以用于覆盖自动检测无法处理的场景
- 例如，在 node 中 @opentelemetry/sdk-trace-node 允许根据特定需求自定义追踪行为，其中 ConsoleSpanExporter 方法可以接收追踪数据（ Spans ），并将这些信息以可读格式输出到控制台（ Console ），通常用于开发和调试

#### 2.3 数据导出

- 收集的遥测数据需要被发送到一个后端系统如分析平台或监控数据库以便进一步处理。这就是数据导出的作用
- OpenTelemetry 通过多种导出器（ exporters ）来支持将数据发送到多种后端。导出器能够将收集的数据转换为特定格式，并通过网络将数据传输到监控系统，如 Prometheus、VictoriaMetrics、Jaeger 或 ElasticSearch 等
- 在某些情况下，OpenTelemetry Collector 也被用来作为一个独立的服务，它可以接收、处理和导出遥测数据。Collector 提供了丰富的处理功能，比如批处理、数据筛选、聚合和转换等
- 使用 OpenTelemetry，开发者可以配置一个或多个导出器，根据需要发送不同类型的遥测数据到不同的后端系统
- 例如，在 node 中 @opentelemetry/exporter-prometheus 包中的 Prometheus 导出器 ( exporter ) 会暴露一个  `/metrics`  的 HTTP 路径。当 Prometheus exporter 被配置并启用后，它会启动一个 HTTP 服务器，该服务器提供一个  `/metrics`  端点，按照 Prometheus 期望的格式返回收集的度量数据

## 3. Prometheus（ VictoriaMetrics 一样 ）

- Prometheus 是系统监控和警报工具包，后来成为了云原生计算基金会（ CNCF ）的一部分。它主要用于收集和存储时间序列数据，并且提供了强大的查询语言 PromQL 来访问这些数据

#### 3.1 数据接收

- 拉取（ Pull ）模式:
  - Prometheus 默认使用拉取模式从预定义的目标收集时间序列数据。它会定期发送 HTTP 请求到这些目标的  `/metrics`  端点获取最新指标数据
  - 这样监控的目标配置集中在 Prometheus 服务器上，便于管理，并且不需要每个监控目标实现复杂的推送逻辑
- 推送（ Push ）模式:
  - 对于那些无法定期拉取的情景，如批处理作业或者短暂的服务（作业的生命周期可能在 Prometheus 拉取周期之外结束），或者应用位于一个复杂网络环境之内，比如火墙后面或 NAT 网络，Prometheus 服务器请求不到，此时 pull 模式可能难以实现
  - Prometheus 提供了一个名为 Pushgateway 的中间组件。应用可以将指标主动推送到 Pushgateway，然后 Prometheus 会从 Pushgateway 中拉取这些数据
- 服务发现: Prometheus 支持多种服务发现机制，如 Kubernetes、Consul、DNS 等，能自动发现目标并从这些目标处抓取指标数据

#### 3.2 数据存储

- 本地存储: Prometheus 采用自带的时间序列数据库本地存储度量数据。这个数据库基于磁盘存储，并且使用了一种高效的数据压缩算法来减少存储占用
- 数据留存: Prometheus 允许配置数据的留存时间，也就是数据能保存多久。超出留存时间的数据会自动被删除
- 远程存储: 尽管 Prometheus 设计为一个单体系统，但也提供了远程存储功能，允许将数据写入到如 InfluxDB、Cortex、Thanos 或 VictoriaMetrics 这样的远程时间序列数据库中

####数据查询

- PromQL: Prometheus 的 PromQL 是一种强大的查询语言，允许用户编写详细的查询来检索度量数据。查询可以用来筛选标签、计算时间范围内的聚合、对数据执行数学运算等
- API: Prometheus 同时提供 HTTP API，方便自定义的应用程序或其他工具直接查询度量数据
- Grafana 集成: 利用 Grafana，用户可以建立可视化仪表板来显示查询结果。Prometheus 通常配合 Grafana 使用，利用其数据源插件直接与 Prometheus 的 API 接口相连

## 4. Grafana

- Grafana 从 Prometheus、VictoriaMetrics 等数据源读取数据的方式是基于  pull 模型的。Grafana 本身不存储任何监控数据，而是按需从配置的数据源拉取数据以进行可视化
- Grafana 不需要实时地接收数据；它只在用户查看仪表板或按照设定的刷新间隔时请求最新的数据。这种按需获取数据的方式有利于实现高效的带宽使用和数据刷新策略

#### 4.1 配置数据源

- Grafana 配置数据源步骤（ Prometheus 为例，因为 VictoriaMetrics 提供了与 Prometheus 兼容的 API，因此和 Prometheus 的配置几乎一模一样 ）：
  1. 添加数据源：在 Grafana 的 UI 中，你需要进入 'Configuration'（ 配置 ） 菜单，点击 'Data Sources'（ 数据源 ）选项，然后点击 'Add data source'（ 添加数据源 ）按钮
  2. 选择 Prometheus：在添加数据源时，会有许多选项。你应该选择 'Prometheus' 作为数据源类型
  3. 配置数据源详情：填写必要的信息，如 Prometheus 服务器的 URL。如果 Prometheus 与 Grafana 部署在同一网络中，这个 URL 就是 Prometheus 服务器的内部网络地址。如果 Grafana 是通过互联网访问 Prometheus，确保 Prometheus 的端点是可访问的，并具备适当的安全措施（ 如通过 HTTPS，可能需要认证信息 ）
  4. 保存&测试：配置完成后，点击 'Save & Test'（ 保存并测试 ）按钮以确保 Grafana 能够成功地连接到 Prometheus 数据源
  5. 创建仪表盘和面板：数据源添加并测试成功之后，你可以开始创建仪表盘和面板来查询和可视化 Prometheus 的数据

#### 4.2 读取数据源

- 整个流程的具体步骤
  1. 用户交互：当用户访问 Grafana 仪表板，或者当仪表板自动刷新来显示最新数据时，Grafana 会根据用户设置的查询和时间范围向数据源发起 HTTP 请求
  2. 查询发送：Grafana 使用数据源提供的查询语言和 API 组织查询。对于 Prometheus，这会是 PromQL 语句；对于与 Prometheus 兼容的 VictoriaMetrics，查询方式相同。Grafana 将这些请求发送到数据源的 HTTP API。Prometheus 和 VictoriaMetrics 都提供了 HTTP 服务接口来接收查询请求，并返回查询结果
  3. 数据返回：数据源收到查询后，执行查询并从其存储中检索相关的时间序列数据，然后将查询结果以可识别的格式（ 通常是 JSON 格式 ）返回给 Grafana
  4. 渲染可视化：Grafana 接收到从数据源返回的数据后，将其处理并在 Web 前端用图形化的方式呈现出来

## 5. 实现监控和可视化工作流（ Opentelemetry + VictoriaMetrics + Grafana ）

#### 5.1 创建 VictoriaMetrics 配置

- 定义如何收集（ "scrape"，即拉取 metrics ）监控数据

```yml
# prometheus.yml #

global: # 定义所有 scrape job 的默认配置
  scrape_interval: 1s # 表示每秒会自动拉取一次所有配置好的 targets 中的监控数据

scrape_configs: # 定义多个监控任务
  - job_name: dubbo_provider # 使用 job_name 来区分不同的数据抓取任务
    metrics_path: /metrics # 表示监控目标暴露 metrics 的路径是 /metrics
    static_configs: # 提供一个或多个已知的固定目标，这些目标的 IP 地址和端口不会频繁变化，因此可以在配置文件中直接指定
      - targets: # 包含具体要监控的目标地址
          - 'host.docker.internal:9464'
  - job_name: dubbo_customer
    metrics_path: /metrics
    static_configs:
      - targets:
          - 'host.docker.internal:9465'
```

#### 5.2 创建 Docker 配置并启动

- 定义 Grafana 和 VictoriaMetrics 服务

```yml
# docker-compose.yml

version: '3.1' # Docker Compose 文件格式的版本，告诉解释器应该如何解析和处理该文件
services: # 定义应用程序包含哪些服务
  grafana:
    image: grafana/grafana-enterprise:10.0.0 # grafana-enterprise 镜像提供了一些额外的企业级特性和插件
    hostname: grafana # 为 Grafana 容器设置的主机名是 grafana
    networks:
      - dubbo-network # 定义了 Grafana 使用 `dubbo-network` 网络，使得在同一网络中的其他服务（如 VictoriaMetrics）可以互相发现和通信
    volumes:
      - ./grafana-data:/var/lib/grafana # 将主机上的 ./grafana-data 目录挂载到容器内的 /var/lib/grafana 目录，这样可以持久化 Grafana 的配置和数据库文件
    ports:
      - "13000:3000" # 将容器内运行的 Grafana 的默认端口 3000 映射到主机的 13000 端口上，使得可以从主机或外部网络访问 Grafana

  vm:
    image: victoriametrics/victoria-metrics:latest
    hostname: vm
    command: # 定义了 VictoriaMetrics 的启动命令和参数。这包括设置数据存储路径 `-storageDataPath /victoria-metrics-data`，数据保留期 `-retentionPeriod 180d`，Prometheus 配置文件的位置 `-promscrape.config /prometheus.yml`，以及服务发现检查间隔 `-promscrape.httpSDCheckInterval 60s`
      -storageDataPath /victoria-metrics-data -retentionPeriod 180d -promscrape.config /prometheus.yml -promscrape.httpSDCheckInterval 60s
    volumes: # 挂载两个目录，一个是包含 Prometheus 配置的 `./prometheus.yml` 文件，另一个是用于持久化
      - ./prometheus.yml:/prometheus.yml
      - ./victoria-metrics-data:/victoria-metrics-data
      VictoriaMetrics 数据的 `./victoria-metrics-data`
    networks:
      - dubbo-network
    ports:
      - "18428:8428"

networks:
  dubbo-network: # 定义了一个自定义 Docker 网络，名为 dubbo-network，在这个网络中，Grafana 和 VictoriaMetrics 服务可以相互通信，且从外部网络隔离
```

- 启动 Grafana 和 VictoriaMetrics 服务器

  1. 启动 Docker

  ```shell
  docker compose up -d
  ```

  2. 访问 Grafana 控制台 http://localhost:13000 并添加数据源

#### 5.3 在服务中注入 Opentelemetry 框架

- 常用依赖

```
# Node.js 的 api 抽象（ 一般用于自定义检测、收集和导出操作 ）
$ npm install --save @opentelemetry/api

# Node.js 的 api 实现
$ npm install --save @opentelemetry/sdk-node

# 常用 Node.js 模块的埋点实现
$ npm install --save @opentelemetry/auto-instrumentations-node

# jaeger 输出器或 prometheus 输出器
$ npm install --save @opentelemetry/exporter-jaeger
$ npm install --save @opentelemetry/exporter-prometheus
```

- 默认埋点行为

```
import { PrometheusExporter } from '@opentelemetry/exporter-prometheus'
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node'
// 用于将追踪数据输出到控制台进行调试
import { ConsoleSpanExporter } from '@opentelemetry/sdk-trace-node'

const sdk = new NodeSDK({
  enable: true,
  configuration: {
    serviceName: "dubbo-observable-example",
    instrumentations: [getNodeAutoInstrumentations()],
    traceExporter: new ConsoleSpanExporter(),
    metricReader: new PrometheusExporter({
      port: 9465
    })
  }
})
sdk.start()
```

- 自定义埋点

```
// 用 Meter 对象去存储数据仪表
// 用 Meter 身上的 Counter 对象去具体存储 请求总数，请求失败数，请求成功数 等等
import type { Meter, Counter } from "@opentelemetry/api"

export class MeterCollector {

  private readonly meter: Meter | undefined;
  ...
  private requestTotal: Counter | undefined;
  ...
}

// QPS 的计数可以通过自定义 counter（map 结构），key 是时间戳 / 1000（得到当前秒），value 是 QPS 值
// 可以通过 setInterval 清空 5 秒前的 map 数据
this.intervalId = setInterval(() => {
  // Clear the counter from 5 seconds ago
  const cs = this.currentSecond();
  Object.keys(this.counter).forEach(key => {
    if (Number(key) < cs - 5) {
      delete this.counter[key];
    }
  })
}, 1000);

this.counter[new Date().getTime() / 1000 - 1] // 获取 上一秒的 QPS
```

- 最后在请求的时候，添加请求总数和 QPS 数，请求成功则添加请求成功数，请求失败则添加请求失败数
