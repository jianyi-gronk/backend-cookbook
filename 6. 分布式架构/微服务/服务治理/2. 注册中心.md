## 1. Dubbo2（ 接口级服务发现 ）

#### 1.1 服务注册

- Dubbo 从设计之初就内置了服务地址发现的能力，Provider 注册地址到注册中心，Consumer 通过订阅实时获取注册中心的地址更新，在收到地址列表后，consumer 基于特定的负载均衡策略发起对 provider 的 RPC 调用
  ![image](https://github.com/user-attachments/assets/60633f3a-ef05-442b-ad22-b06adfdd5651)

#### 1.2 举例 Zookeeper 结构

- Zookeeper 结构
  ![image](https://github.com/user-attachments/assets/27e67897-7c7d-4ad6-8b98-fc096ad7b883)
- 流程
  - 服务提供者启动时：向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址
  - 服务消费者启动时：订阅 /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向 /dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址
  - 监控中心启动时：订阅 /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址
- 支持以下功能
  - 当提供者出现断电等异常停机时，注册中心能自动删除提供者信息
  - 当注册中心重启时，能自动恢复注册数据，以及订阅请求
  - 当会话过期时，能自动恢复注册数据，以及订阅请求
  - 当设置 <dubbo:registry check="false" /> 时，记录失败注册和订阅请求，后台定时重试
  - 可通过 <dubbo:registry username="admin" password="1234" /> 设置 zookeeper 登录信息
  - 可通过 <dubbo:registry group="dubbo" /> 设置 zookeeper 的根节点，不配置将使用默认的根节点。
  - 支持 \* 号通配符 <dubbo:reference group="\*" version="\*" />，可订阅服务的所有分组和所有版本的提供者

#### 1.3 具体结构

- 接口级地址发现的内部数据结构
  ![image](https://github.com/user-attachments/assets/4f0ae232-8fa2-4fd6-a130-d94abd25c2eb)
- 右下角 provider 实例内部的数据与行为
  - Provider 部署的应用中通常会有多个 service，也就是 Dubbo2 中的服务，每个 service 都可能会有其独有的配置
  - 所讲的 service 服务发布的过程，其实就是基于这个服务配置生成地址 URL 的过程，生成的地址数据如图所示。同样的，其他服务也都会生成地址
- 注册中心的地址数据存储结构
  - 注册中心以 service 服务名为数据划分依据，将一个服务下的所有地址数据都作为子节点进行聚合，子节点的内容就是实际可访问的 ip 地址，也就是 Dubbo 中 URL，格式就是刚才 provider 实例生成的

#### 1.4 URL 地址数据

- URL 地址数据结构
  ![image](https://github.com/user-attachments/assets/4512ffd3-7a44-4cb7-a462-b06676ceee03)
- 把 URL 地址数据划分成了几份：
  - 首先是实例可访问地址，主要信息包含 ip port，是消费端将基于这条数据生成 tcp 网络链接，作为后续 RPC 数据的传输载体
  - 其次是 RPC 元数据，元数据用于定义和描述一次 RPC 请求，一方面表明这条地址数据是与某条具体的 RPC 服务有关的，它的版本号、分组以及方法相关信息，另一方面表明
  - 下一部分是 RPC 配置数据，部分配置用于控制 RPC 调用的行为，还有一部分配置用于同步 Provider 进程实例的状态，典型的如超时时间、数据编码的序列化方式等
  - 最后一部分是自定义的元数据，这部分内容区别于以上框架预定义的各项配置，给了用户更大的灵活性，用户可任意扩展并添加自定义元数据，以进一步丰富实例状

#### 1.5 Dubbo2 总结

- 结合以上对于 Dubbo2 接口级地址模型的分析，以及最开始的 Dubbo 基本原理图，可以得出这么几条结论：
  - 第一，地址发现聚合的 key 就是 RPC 粒度的服务
  - 第二，注册中心同步的数据不止包含地址，还包含了各种元数据以及配置
  - 得益于 1 与 2，Dubbo 实现了支持应用、RPC 服务、方法粒度的服务治理能力
- 这就是一直以来 Dubbo2 在易用性、服务治理功能性、可扩展性上强于很多服务框架的真正原因

## 2. 接口级服务发现 的缺点

#### 2.1 出现问题

- Dubbo2 地址模型带来易用性和强大功能的同时，也给整个架构的水平可扩展性带来了一些限制
- 这个问题在普通规模的微服务集群下是完全感知不到的，而随着集群规模的增长，当整个集群内应用、机器达到一定数量时，整个集群内的各个组件才开始遇到规模瓶颈
- 在总结包括阿里巴巴、工商银行等多个典型的用户在生产环境特点后，总结出以下两点突出问题（ 如图中红色所示 ）：
  ![image](https://github.com/user-attachments/assets/33daff85-07f9-42ce-9f0e-6477feaeaea8)
  - 首先，注册中心集群容量达到上限阈值。由于所有的 URL 地址数据都被发送到注册中心，注册中心的存储容量达到上限，推送效率也随之下降
  - 而在消费端这一侧，Dubbo2 框架常驻内存已超 40%，每次地址推送带来的 cpu 等资源消耗率也非常高，影响正常的业务调用。

#### 2.2 问题原因

- 以一个具体 provider 示例进行展开，来尝试说明为何应用在接口级地址模型下容易遇到容量问题
- 上图青蓝色部分，假设这里有一个普通的 Dubbo Provider 应用，该应用内部定义有 10 个 RPC Service，应用被部署在 100 个机器实例上。这个应用在集群中产生的数据量将会是 “Service 数 \* 机器实例数”，也就是 10 \* 100 = 1000 条。数据被从两个维度放大：
  - 从地址角度。100 条唯一的实例地址，被放大 10 倍
  - 从服务角度。10 条唯一的服务元数据，被放大 100 倍

## 3. Dubbo3（ 应用级服务发现 ）

#### 3.1 Dubbo3 原理

- 在 Dubbo3 架构下，我们不得不重新思考两个问题：
  - 在保留易用性、功能性的同时
  - 如何重新组织 URL 地址数据，避免冗余数据的出现，让 Dubbo3 能支撑更大规模集群水平扩容？
  - 如何在地址发现层面与其他的微服务体系如 Kubernetes、Spring Cloud 打通？
- Dubbo3 的应用级服务发现方案设计本质上就是围绕以上两个问题展开，其基本思路是：
  - 地址发现链路上的聚合元素也就是我们之前提到的 Key 由服务调整为应用，这也是其名称叫做应用级服务发现的由来
  - 另外，通过注册中心同步的数据内容上做了大幅精简，只保留最核心的 ip、port 地址数据
- 基本原理
  ![image](https://github.com/user-attachments/assets/577d692d-0912-4cfb-b8ef-4e1a03ea1a57)

#### 3.2 注册中心数据结构

- 升级之后应用级地址发现的内部数据结构
  ![image](https://github.com/user-attachments/assets/233304db-b7ba-468d-91e0-ff474890152d)
- 对比之前接口级的地址发现模型，主要关注橙色部分的变化
  - 首先，在 provider 实例这一侧，相比于之前每个 RPC Service 注册一条地址数据，一个 provider 实例只会注册一条地址到注册中心
  - 而在注册中心这一侧，地址以应用名为粒度做聚合，应用名节点下是精简过后的 provider 实例地址

#### 3.3 服务自省

- 应用级服务发现的上述调整，同时实现了地址单条数据大小和总数量的下降，但同时也带来了新的挑战：
  - 之前 Dubbo2 强调的易用性和功能性的基础损失了，因为元数据的传输被精简掉了，如何精细的控制单个服务的行为变得无法实现
- 针对这个问题，Dubbo3 的解法是提出服务自省，服务自省在服务消费端和提供端之间建立了一条内置的 RPC 服务信息协商机制。服务端实例会暴露一个预定义的 MetadataService RPC 元数据服务，由中心化推送转为 Consumer 到 Provider 的点对点拉取，消费端通过调用 MetadataService 获取每个实例 RPC 方法相关的配置信息
  ![image](https://github.com/user-attachments/assets/85c56592-a0c8-4a43-ab5f-9a349aad1a67)
- 在服务自省模式下，元数据传输的数据量将不在是一个问题，因此可以在元数据中扩展出更多的参数、暴露更多的治理数据
- 工作流程
  ![image](https://github.com/user-attachments/assets/a358fe21-4108-4b12-84e5-49001f381647)
- 总结
  ![image](https://github.com/user-attachments/assets/ddebe77e-7f39-4008-a543-64c6daec1e7a)

#### 3.4 完整工作流程

- 工作流程图
  ![image](https://github.com/user-attachments/assets/bf51f79d-9363-4ad7-8039-6a328e6c7642)
- 消费端 Consumer 的地址订阅行为，消费端从分两步读取地址数据。首先是从注册中心收到精简后的地址，随后通过调用 MetadataService 元数据服务，读取对端的元数据信息
- 在收到这两部分数据之后，消费端会完成地址数据的聚合，最终在运行态还原出类似 Dubbo2 的 URL 地址格式
- 因此从最终结果而言，应用级地址模型同时兼顾了地址传输层面的性能与运行层面的功能性

## 4. Dubbo2 升 Dubbo3 兼容

- 提供者默认同时进行应用级注册和接口级注册，消费者对提供者注册的数据来决定使用应用级发现或者接口级发现
- 提供者双注册
  ![image](https://github.com/user-attachments/assets/13a47ad3-e556-4e17-adef-9e876759cad9)
- 消费者订阅决策
  ![image](https://github.com/user-attachments/assets/4fc220a2-9a33-4558-98bf-48f97f77b5c7)
