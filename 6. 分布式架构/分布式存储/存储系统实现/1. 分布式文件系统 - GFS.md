## 1. 简述 GFS

- 是什么
  - 大规模可扩展容错的分布式文件系统
- 有什么意义
  - 在廉价的服务器上运行，节省服务器成本
  - 随着业务发展，可以水平扩展
  - 容错并自动恢复
  - 处理超大规模数据集
- 特性
  - 容错，运行于普通机器上
  - 大文件数据处理
  - 在文件尾部追加数据
  - 降低一致性预期的 api 设计

## 2. GFS 用于解决了什么场景

- 系统由许多廉价的普通组件组成，组件失效是一种常态
  - GFS 组件的数量和质量导致在事实上，任何给定时间内都有可能发生某些组件无法工作，某些组件无法从它们目前的失效状态中恢复
  - 可能存在各种各样的问题，比如 应用程序 bug、操作系统的 bug、人为失误，甚至还有硬盘、内存、连接器、网络以及电源失效等造成的 问题
  - 系统必须持续监控自身的状态，它必须将组件失效作为一种常态，能够迅速地侦测、冗余并恢复失效的组件
- 大文件普遍存在，但小文件也是存在的
  - 预期会有几百万文件，文件的大小通常在 100MB 或者以上。数个 GB 大小的文件也是普遍存在，并且要能够被有效的管理，采用管理数亿个 KB 大小的小文件的方式是非常不明智的
  - 系统也必须支持小文件，但是不需要针对小文件做专门的优化
  - 因此设计的假设条件和参数，比如 I/O 操作和 Block 的尺寸都需要重新考虑
- 系统的工作负载主要由 **大规模的流式读取** 和 **小规模的随机读取** 这两种读操作组成
  - 大规模的流式读取通常一次读取数百 KB 的数据，更常见的是一次读取 1MB 甚至更多的数据。来自同一个客户机的连续操作通常是读取同一个文件中连续的一个区域
  - 小规模的随机读取通常是在文件某个随机的位置读取几个 KB 数据
  - 如果应用程序对性能非常关注，通常的做法是把小规模的随机读取操作合并并排序，之后按顺序批量读取，这样就避免了在文件中前后来回的移动读取位置
- 系统必须高效的、行为定义明确的实现多客户端并行追加数据到同一个文件里的语义
  - 我们的文件通常被用于 ”生产者-消费者“ 队列，或者其它多路文件合并操作，通常会有数百个生产者，每个生产者进程运行在一台机器上，同时对一个文件进行追加操作
  - 使用最小的同步开销来实现的原子的多路追加数据操作是必不可少的。文件可以在稍后读取，或者是消费者在追加的操作的同时读取文件
- 吞吐优先于延迟
  - 我们的目标程序绝大部分要求能够高速率的、大批量的处理数据，极少有程序对单一的读写操作有严格的响应时间要求

## 3. GFS 暴露给用户哪些核心接口

- 分层组织目录
- 增删，打开，关闭，（ 顺序 ）读写文件
- 快照（ 相当于拷贝 ）
- 原子追加（ 用于处理多个生产者同时操作一个文件的场景 ）

## 4. GFS 架构

<img src="https://img-blog.csdnimg.cn/direct/e7e8e5a636f84d2dbc39724f399c21c1.png">

#### 4.1 GFS 集群

- 一个 GFS 集群包含
  - 一个单独的 Master 节点
    - 所有其他节点的元数据全部存储在一个 master 节点的 **内存** 中（ 1GB 空间可以存储 几百 TB 空间的元数据，所以完全可以放在单节点存储 ）
    - 这里的一个单独的 Master 节点的含义是 GFS 系统中 只存在一个逻辑上的 Master 组件，其实还存在 Master 节点复制，为了理解方便，把 Master 节点视为一个逻辑上的概念
    - 一个逻辑的 Master 节点包括两台物理主机，即两台 Master 服务器
  - 多台 Chunk 服务器
    - 同时可以被多个客户端访问

#### 4.2 master 节点

- 单节点存储所有的文件系统的元数据信息
  - 元数据信息包括
    - 名称空间
      - 即比如 /a/b/text.txt
      - 可以拿一个 map 去存，key 是路径，value 是文件内容
    - 访问控制信息
    - 文件和 Chunk 的映射信息
    - 当前 Chunk 的位置信息（ 存储在哪个 chunck server 上 ）
    - chunck server 在什么位置
    - 位置信息 chunck server 最有决定权，每次都 poll 更新
- 管理着系统范围内的活动
  - 使用心跳信息周期地和每个 Chunk 服务器通讯，发送指令到各个 Chunk 服务器并接收 Chunk 服务器的状态信息
  - Chunk 租用管理
  - 孤儿 Chunk 的回收
  - Chunk 在 Chunk 服务器之间的迁移
  - 并发访问控制

#### 4.3 chunck server 节点

- GFS 存储的文件都被分割成固定大小 64MB 的 Chunk（ 如果不够，用空白数据去填充 ）
  - 在 Chunk 创建的时候，Master 服务器会给每个 Chunk 分配一个不变的、全球唯一的 64 位的 Chunk 标识
  - Chunk 服务器把 Chunk 以 Linux 文件的形式保存在本地硬盘上，并且根据指定的 Chunk 标识和字节范围来读写块数据
  - 出于可靠性的考虑，每个块都会复制到多个块服务器上
    - 复制节点少了，可靠性不够；复制节点多了，难保持复制节点的数据一致性
    - 缺省情况下，我们使用 3 个存储复制节点，不过用户可以为不同的文件命名空间设定不同的复制级别

#### 4.4 客户端节点

- GFS 客户端代码以库的形式被链接到客户程序里
- 客户端代码实现了 GFS 文件系统的 API 接口函数、应用程序与 Master 节点和 Chunk 服务器通讯、以及对数据进行读写操作
- 客户端和 Master 节点的通信只获取元数据，所有的数据操作都是由客户端直接和 Chunk 服务器进行交互的
  - 这样可以减少 Master 单点故障问题，虽然存在备用 Master 节点，但是切换一样是需要消耗时间的
- 不提供 POSIX 标准的 API 的功能，因此 GFS API 调用不需要深入到 Linux vnode 级别

#### 4.5 文件缓存

- 无论是客户端还是 Chunk 服务器都不需要缓存文件数据（ 不过，客户端会缓存元数据 ）
  - 客户端缓存数据几乎没有什么用处，因为大部分程序要么以流的方式读取一个巨大文件，要么工作集太大根本无法被缓存
  - Chunk 服务器不需要缓存文件数据的原因是，Chunk 以本地文件的方式保存，Linux 操作系统的文件系统缓存会把经常访问的数据缓存在内存中
  - 无需考虑缓存相关的问题也简化了客户端和整个系统的设计和实现

## 5. GFS 设计细节

#### 5.1 避免单一的 Master 节点成为系统的瓶颈

- 单一的 Master 节点的策略大大简化了我们的设计，单一的 Master 节点可以通过全局的信息精确定位 Chunk 的位置以及进行复制决策
- 但我们必须减少对 Master 节点的读写，避免 Master 节点成为系统的瓶颈
  - 客户端并不通过 Master 节点读写文件数据
  - 而是客户端向 Master 节点询问它应该联系的 Chunk 服务器
  - 并且客户端将这些元数据信息缓存一段时间，后续的操作将直接和 Chunk 服务器进行数据读写操作
- 一次简单读取的流程
  - 首先，客户端把文件名和程序指定的字节偏移，根据固定的 Chunk 大小，转换成文件的 Chunk 索引
  - 然后，它把文件名和 Chunk 索引发送给 Master 节点
  - Master 节点将相应的 Chunk 标识和副本的位置信息发还给客户端
  - 客户端用文件名和 Chunk 索引作为 key 缓存这些元数据信息
  - 之后客户端发送请求到其中的一个副本处，一般会选择最近的，请求信息包含了 Chunk 的标识和字节范围
  - 在对这个 Chunk 的后续读取操作中，客户端不必再和 Master 节点通讯了，除非缓存的元数据信息过期或者文件被重新打开
    - 实际上，客户端通常会在一次请求中查询多个 Chunk 信息，Master 节点的回应也可能包含了紧跟着这些被请求的 Chunk 后面的 Chunk 的信息
    - 在实际应用中，这些额外的信息在没有任何代价的情况下，避免了客户端和 Master 节点未来可能会发生的几次通讯

#### 5.2 Chunk 尺寸设计

- Chunk 的大小是关键的设计参数之一，通常选择 64MB，这个尺寸远远大于一般文件系统的 Block size
  - 每个 Chunk 的副本都以普通 Linux 文件的形式保存在 Chunk 服务器上，只有在需要的时候才扩大
  - 惰性空间分配策略避免了因内部碎片造成的空间浪费，内部碎片或许是对选择这么大的 Chunk 尺寸最具争议一点。
- 选择较大的 Chunk 尺寸有几个重要的优点
  - 首先，它减少了客户端和 Master 节点通讯的需求，因为只需要一次和 Mater 节点的通信就可以获取 Chunk 的位置信息，之后就可以对同一个 Chunk 进行多次的读写操作
    - 这种方式对降低我们的工作负载来说效果显著，因为我们的应用程序通常是连续读写大文件。即使是小规模的随机读取，采用较大的 Chunk 尺寸也带来明显的好处，客户端可以轻松的缓存一个数 TB 的工作数据集所有的 Chunk 位置信息
  - 其次，采用较大的 Chunk 尺寸，客户端能够对一个块进行多次操作，这样就可以通过与 Chunk 服务器保持较长时间的 TCP 连接来减少网络负载
  - 并且，选用较大的 Chunk 尺寸减少了 Master 节点需要保存的元数据的数量，这就允许我们把元数据全部放在 Master 节点的内存中，元数据全部放在内存中会带来些额外的好处
- 另一方面，即使配合惰性空间分配，采用较大的 Chunk 尺寸也有其缺陷
  - 小文件包含较少的 Chunk，甚至只有一个 Chunk。当有许多的客户端对同一个小文件进行多次的访问时，存储这些 Chunk 的 Chunk 服务 器就会变成热点
  - 在实际应用中，由于我们的程序通常是连续的读取包含多个 Chunk 的大文件，热点还不是主要的问题
- 然而，当 GFS 第一次被批处理队列系统使用时，热点确实出现了
  - 一个可执行文件被作为单个 chunkfile 写入 GFS，然后同时在数百台机器上启动，存储这个可执行文件的少数 chunkserver 被数百个同时请求重载
  - 我们通过以更高的复制因子存储这些可执行文件，并通过使批处理队列系统错开应用程序启动时间来解决这个问题
  - 一个潜在的长期解决方案是允许客户端在这种情况下从其他客户端读取数据

#### 5.3 元数据内容

- Master 服务器存储 3 种主要类型的元数据，所有的元数据都保存在 Master 服务器的内存中
  - 文件和 Chunk 的命名空间
  - 文件和 Chunk 的对应关系
  - 每个 Chunk 副本的存放地点
- 前两种类型的元数据同时也会以记录变更日志的方式记录在操作系统的系统日志文件中，日志文件存储在本地磁盘上，同时日志会被复制到其它的远程 Master 服务器上
  - 采用保存变更日志的方式，我们能够简单可靠的更新 Master 服务器 的状态，并且不用担心 Master 服务器崩溃导致数据不一致的风险
  - Master 服务器不会持久保存 Chunk 位置信息。Master 服务器在启动时，或者有新的 Chunk 服务器加入时，向各个 Chunk 服务器轮询它们所存储的 Chunk 的信息
- 因为元数据保存在内存中，所以 Master 服务器的操作速度非常快
  - 并且，Master 服务器可以在后台简单而高效的周期性扫描自己保存的全部状态信息
  - 这种周期性的状态扫描也用于实现 Chunk 垃圾收集；在 Chunk 服务器失效的时重新复制数据；通过 Chunk 的迁移实现跨 Chunk 服务器的负载均衡以及磁盘使用状况统计等功能
- 将元数据全部保存在内存中的方法有潜在问题，即 Chunk 的数量以及整个系统的承载能力都受限于 Master 服务器所拥有的内存大小，但是在实际应用中，这并不是一个严重的问题
  - Master 服务器只需要不到 64 个字节的元数据就能够管理一个 64MB 的 Chunk，由于大多数文件都包含多个 Chunk，因此绝大多数 Chunk 都是满的，除了文件的最后一个 Chunk 是部分填充的
  - 同样的，每个文件在命名空间中的数据大小通常在 64 字节以下，因为保存的文件名是用前缀压缩算法压缩过的
  - 即便是需要支持更大的文件系统，为 Master 服务器增加额外内存的费用是很少的，而通过增加有限的费用，就能够把元数据全部保存在内存里，增强了系统的简洁性、可靠性、高性能和灵活性

#### 5.4 Chunk 位置信息

- Master 服务器 **并不持久化保存** 哪个 Chunk 服务器存有指定 Chunk 的副本的信息
  - Master 服务器只是在启动的时候轮询 Chunk 服务器以获取这些信息
  - Master 服务器能够保证它持有的信息始终是最新的，因为它控制了所有的 Chunk 位置的分配，而且通过周期性的心跳信息监控 Chunk 服务器的状态
- 最初设计时，试图把 Chunk 的位置信息持久的保存在 Master 服务器上，但是后来发现在启动的时候轮询 Chunk 服务器，之后定期轮询更新的方式更简单
  - 这种设计简化了在有 Chunk 服务器加入集群、离开集群、更名、失效、以及重启的时候，Master 服务器和 Chunk 服务器数据同步的问题。在一个拥有数百 台服务器的集群中，这类事件会频繁的发生
- 可以从另外一个角度去理解这个设计决策
  - 只有 Chunk 服务器才能最终确定一个 Chunk 是否在它的硬盘上
  - 我们从没有考虑过在 Master 服务器上维护一个这些信息的全局视图，因为 Chunk 服务器的错误可能会导致 Chunk 自动消失（ 比如，硬盘损坏了或者无法访问了 ），亦或者操作人员可能会重命名一个 Chunk 服务器

#### 5.5 操作日志

- 操作日志包含了关键的元数据变更历史记录，这对 GFS 非常重要。这不仅仅是因为操作日志是元数据唯一的持久化存储记录，它也作为判断同步操作顺序的逻辑时间基线（ 也就是通过逻辑日志的序号作为操作发生的逻辑时间，类似于事务系统中的 LSN ）。文件和 Chunk，连同它们的版本，都由它们创建的逻辑时间唯一的、永久的标识
- 操作日志非常重要，我们必须确保日志文件的完整，确保只有在元数据的变化被持久化后，日志才对客户端是可见的。否则，即使 Chunk 本身没有出现任何问题，我们仍有可能丢失整个文件系统，或者丢失客户端最近的操作。所以，我们会把日志复制到多台远程机器，并且只有把相应的日志记录写入到本地以及远程机器的硬盘后，才会响应客户端的操作请求。Master 服务器会收集多个日志记录后批量处理，以减少写入磁盘和复制对系统整体性能的影响
- Master 服务器在灾难恢复时，通过重演操作日志把文件系统恢复到最近的状态。为了缩短 Master 启动的时间，我们必须使日志足够小（ 即重演系统操作的日志量尽量的少 ）。Master 服务器在日志增长到一定量时，对系统状态做一次 Checkpoint（ 即对数据库状态作一次快照 ），将所有的状态数据写入一个 Checkpoint 文件，并删除之前的日志文件。 在灾难恢复的时候，Master 服务器就通过从磁盘上读取这个 Checkpoint 文件，以及重演 Checkpoint 之后的有 限个日志文件就能够恢复系统。Checkpoint 文件以压缩 b-tree 形势的数据结构存储，可以直接映射到内存，在用于命名空间查询时无需额外的解析。这大大提高了恢复速度，增强了可用性
- 由于创建一个 Checkpoint 文件需要一定的时间，所以 Master 服务器的内部状态被组织为一种格式，这种 格式要确保在 Checkpoint 过程中不会阻塞正在进行的修改操作。Master 服务器使用独立的线程切换到新 的日志文件和创建新的 Checkpoint 文件。新的 Checkpoint 文件包括切换前所有的修改。对于一个包含数 百万个文件的集群，创建一个 Checkpoint 文件需要 1 分钟左右的时间。创建完成后，Checkpoint 文件会被写入在本地和远程的硬盘里
- Master 服务器恢复只需要最新的 Checkpoint 文件和后续的日志文件。旧的 Checkpoint 文件和日志文件可以被删除，但是为了应对灾难性的故障，我们通常会多保存一些历史文件。Checkpoint 失败不会对 正确性产生任何影响，因为恢复功能的代码可以检测并跳过没有完成的 Checkpoint 文件

## 6. 一致性模型

- GFS 支持一个宽松的一致性模型，这个模型能够很好的支撑我们的高度分布的应用，同时还保持了相对简单且容易实现的优点
- 文件命名空间的修改（ 例文件创建 ）是原子性的，它们仅由 Master 节点的控制
  - 命名空间锁提供了原子性和正确性的保障
  - Master 节点的操作日志定义了这些操作在全局的顺序
- 数据修改后文件 region（ 即修改操作所涉及的文件中的某个范围 ）的状态取决于 操作的类型，成功与否，以及 是否同步修改，通常情况下，文件 region 内包含了来自多个修改操作的、混杂的数据片段
  - 如果所有客户端，无论从哪个副本读取，读到的数据都一样，那么我们认为文件 region 是 “一致的”
  - 如果对文件的数据修改之后，region 是一致的，并且客户端能够看到写入操作全部的内容，那么这个 region 是 “已定义的”
- 下图总结了各种操作的结果
  <img src="https://img-blog.csdnimg.cn/direct/f9d7e73d7607416e9a30fc7c761f31e9.png">
- 当一个数据修改操作成功执行，并且没有受到同时执行的其它写入操作的干扰，那么影响的 region 就是已定义的（ 隐含了一致性 ），即所有的客户端都可以看到写入的内容
- 并行修改操作成功完成之后，region 处于一致的、未定义的状态，即所有的客户端看到同样的数据，但是无法读到任何一次写入操作写入的数据
- 失败的修改操作导致一个 region 处于不一致状态（ 同时也是未定义的 ），即不同的客户在不同的时间会看到不同的数据

#### 6.1 数据修改

- 数据修改操作分为写入或者记录追加两种
  - 写入操作把数据写在应用程序指定的文件偏移位置上
  - 即使有多个修改操作并行执行时，记录追加操作至少可以把数据原子性的追加到文件中一次，但是偏移位置是由 GFS 选择的（ 即所有的追加写入都会成功，但是有可能被执行了多次，而且每次追加的文件偏移量由 GFS 自己计算 ）。(相比而言，通常说的追加操作写的偏移位置 是文件的尾部。)GFS 返回给客户端一个偏移量，表示了包含了写入记录的、已定义的 region 的起点。另外，GFS 可能会在文件中间插入填充数据或者重复记录。这些数据占据的文件 region 被认定是不一致的， 这些数据通常比用户数据小的多。
    经过了一系列的成功的修改操作之后，GFS 确保被修改的文件 region 是已定义的，并且包含最后一次修改操作写入的数据。GFS 通过以下措施确保上述行为:(a) 对 Chunk 的所有副本的修改操作顺序一致 (3.1 章)，(b)使用 Chunk 的版本号来检测副本是否因为它所在的 Chunk 服务器宕机(4.5 章)而错过了修改操作而导致其失效。失效的副本不会再进行任何修改操作，Master 服务器也不再返回这个 Chunk 副本的位置信息给客户端。它们会被垃圾收集系统尽快回收。
    由于 Chunk 位置信息会被客户端缓存，所以在信息刷新前，客户端有可能从一个失效的副本读取了数据。 在缓存的超时时间和文件下一次被打开的时间之间存在一个时间窗，文件再次被打开后会清除缓存中与该文件有关的所有 Chunk 位置信息。而且，由于我们的文件大多数都是只进行追加操作的，所以，一个失效的副本通常返回一个提前结束的 Chunk 而不是过期的数据。当一个 Reader(alex 注:本文中将用到两个 专有名词，Reader 和 Writer，分别表示执行 GFS 读取和写入操作的程序)重新尝试并联络 Master 服务器时，它就会立刻得到最新的 Chunk 位置信息。
    即使在修改操作成功执行很长时间之后，组件的失效也可能损坏或者删除数据。GFS 通过 Master 服务器和 所有 Chunk 服务器的定期“握手”来找到失效的 Chunk 服务器，并且使用 Checksum 来校验数据是否损坏 (5.2 章)。一旦发现问题，数据要尽快利用有效的副本进行恢复(4.3 章)。只有当一个 Chunk 的所有副本在 GFS 检测到错误并采取应对措施之前全部丢失，这个 Chunk 才会不可逆转的丢失。在一般情况下 GFS 的反应时间(alex 注:指 Master 节点检测到错误并采取应对措施)是几分钟。即使在这种情况下，Chunk 也只是不可用了，而不是损坏了:应用程序会收到明确的错误信息而不是损坏的数据。
