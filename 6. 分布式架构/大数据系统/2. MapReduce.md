## 1. MapReduce 解决了什么问题

- 2004 年谷歌提出了 MapReduce，在此之前谷歌程序员面对的大规模数据集，常常需要编程实现，比如
  - 统计某个关键词的现的频率，计算 pageRank
  - 对大规模数据按词频排序
  - 对多台机器上的文件进行 grep 等
- 因为是大规模，所以这些工作不可能在一台机器上完成，因此谷歌的程序员每次编写代码都需要处理，多机并行协同，网络通信，处理错误，提高执行效率等问题
- 这些问题使得开发效率严重降低，因此为了治理这一现象导致的复杂度，Jeff Dean，设计了一种新的编程模型 MapReduce
- 作为一个成熟的工业级实现 MapReduce 就是一个利用普通机器组成的大规模计算集群进行并行的，高容错，高性能的数据处理函数框架，为解决上述多机并行协同，网络通信，处理错误，提高执行效率等通用性问题

## 2. 简述 MapReduce 是什么

- MapReduce 本质就是由两个函数来处理大规模数据（ Map 和 Reduce 函数 ）

  - 输入一组 key 和 value 到 Map 函数，处理后输出多组 key 和 value
  - 将一个 key 对应的所有 value 同时输入到一个 Reduce 函数中执行，处理后输出最终结果
  - map 和 reduce 都是可以各自并行执行，但 reduce 执行的前提条件是所有的 map 函数执行完毕
  - 简单 Map 和 Reduce 函数

    ```
    map(String key,String value):
      // key: 文档名
      // value: 文档内容
      for each word w in value:
        EmitIntermediate(w, "1");

    reduce(Stringkey, Iterator values):
      // key: 一个单词
      // value: 计数值列表
      int result = 0;
      for each v in values:
        result += ParseInt(v);
      Emit(AsString(result));
    ```

## 3. MapReduce 详细流程

<img src="https://img-blog.csdnimg.cn/direct/7c64ce3eb99841d2b8b34da71160e4c6.png">

1. 用户程序中的 MapReduce 库首先将输入文件切分为 M 块数据片段，每块的大小从 16MB 到 64MB（ 用户可通过一个可选参数控制此大小 ），然后 MapReduce 库会在一个集群的若干台机器上启动程序的多个副本
2. 程序的各个副本中有一个是特殊的，即主节点，其它的则是工作节点。主节点将 M 个 map 任务和 R 个 reduce 任务分配给空闲的工作节点，每个节点一项任务
   - 主节点维持多种数据结构。它会存储每个 map 和 reduce 任务的状态（ 空闲、处理中、完成 ），和每台工作机器的 ID（ 对应非空闲的任务 ）
   - 主节点是将 map 任务产生的中间文件的位置传递给 reduce 任务的通道。因此，主节点要存储每个已完成的 map 任务产生的 R 个中间文件的位置和大小。位置和大小信息的更新情况会在 map 任务完成时接收到。这些信息会被逐步发送到正在处理中的 reduce 任务节点处
3. 被分配 map 任务的工作节点读取对应的输入区块内容。它从输入数据中解析出 key / value 对，然后将每个对传递给用户定义的 map 函数。由 map 函数产生的中间 key / value 对都缓存在内存中
4. 缓存的数据对由划分函数分成 R 块，会被周期性地写入本地磁盘中。这些缓存对在本地磁盘中的位置会被传回给主节点，主节点负责将这些位置再传给 reduce 工作节点
   - 划分函数按 map 函数输出的 key 决定当前的 kv 被划分到哪一个中间文件中
   - 工程经验上看,我们有时不仅仅要求其平衡划分，我们还可能要求其可以按照某种 kv 的元数据信息进行分区。比如将一个主机下的文件划分到一起等定制的能力
5. 当一个 reduce 工作节点得到了主节点的这些位置通知后，它使用 RPC 调用去读 map 工作节点的本地磁盘中的缓存数据。当 reduce 工作节点读取完了所有的中间数据，它会将这些数据按中间 key 排序，这样相同 key 的数据就被排列在一起了。同一个 reduce 任务经常会分到有着不同 key 的数据，因此这个排序很有必要。如果中间数据数量过多，不能全部载入内存，则会使用外部排序
6. reduce 工作节点遍历排序好的中间数据，并将遇到的每个中间 key 和与它关联的一组中间 value 传递给用户的 reduce 函数。reduce 函数的输出会写到由 reduce 划分过程划分出来的最终输出文件的末尾
7. 当所有的 map 和 reduce 任务都完成后，主节点唤醒用户程序。此时，用户程序中的 MapReduce 调用返回到用户代码中

## 4. MapReduce 的应用场景

- 分布式 Grep：
  - map 函数在匹配到给定的 pattern 时输出一行
  - reduce 函数只是将给定的中间数据复制到输出上
- URL 访问频次统计：
  - map 函数处理网页请求的日志，对每个 URL 输出 ( URL, 1 )
  - reduce 函数将相同 URL 的所有值相加并输出 ( URL, 总次数 ) 对
- 倒转 Web 链接图：
  - map 函数在 source 页面中针对每个指向 target 的链接都输出一个 ( target, source ) 对
  - reduce 函数将与某个给定的 target 相关联的所有 source 链接合并为一个列表，并输出 ( target, list(source) ) 对
- 每个主机的关键词向量：
  - 关键词向量是对出现在一个文档或一组文档中的最重要的单词的概要，其形式为〈单词, 频率〉对
  - map 函数针对每个输入文档（ 其主机名可从文档 URL 中提取到 ）输出一个 ( 主机名, 关键词向量 ) 对，给定主机的所有文档的关键词向量都被传递给 reduce 函数
  - reduce 函数将这些关键词向量相加，去掉其中频率最低的关键词，然后输出最终的 ( 主机名, 关键词向量 ) 对
- 倒排索引：
  - map 函数解析每个文档，并输出一系列 ( 单词, 文档 ID ) 对
  - reduce 函数接受给定单词的所有中间对，将它们按文档 ID 排序，再输出 ( 单词, list(文档 ID) ) 对。所有输出对的集合组成了一个简单的倒排索引。用户可以很轻松的扩展这个过程来跟踪单词的位置
- 分布式排序：
  - map 函数从每条记录中提取出 key，并输出 ( key, 记录 ) 对
  - reduce 函数不改变这些中间对，直接输出

## 5. 如何高容错优化

#### 5.1 执行 map 任务节点的容错

- 通过周期性的 ping-pong 心跳机制 main 节点感知到 map 节点的状态，如果心跳超时认为节点失败。重新将当前 worker 上的 map task 传递给其他 worker 节点完成。主节点要维护一个任务队列来记录哪些任务还未完成，同时记录已经完成的任务的状态，来感知到当前 mapreduce 任务处于一个怎样的生命周期

#### 5.2 执行 reduce 任务节点的容器

- reduce 产出的文件是一个持久性的文件，存在副作用，因此每次 reduce 被重新分配后要重命名一个新的文件，防止与损坏的文件冲突

#### 5.3 主节点的容错

- 对于任务执行的元数据产生的中间态可以保持在一个恢复点文件中，当节点崩溃重启后可以从最近的一个恢复点重新执行 mapreduce 任务

#### 5.4 副作用

- 对于所有持久化的操作，不可避免的会产生副作用。导致这种副作用的根本原因在于不能原子的提交状态
- 因此解决方案就是保证 map 与 reduce 产生的文件在执行过程中先存储在临时文件中（ 以时间戳命名 ）等到提交文件时 将其原子的重命名为最终文件 （ linux 内核中重命名操作是有原子性的保证的 ）

## 6. 如何高性能优化

#### 6.1 利用局部性

- 在分布式系统中，网络带宽通常是稀缺资源，很容易成为系统瓶颈
- 在 MapReduce 的任务中 至少需要 M \* R 次的网络传输，才能将中间文件发送给 reduce 所在的 worker 节点上。同时把输入文件发送给 map 任务所在的 worker 也是非常消耗网络带宽的事情
- 我们可以通过仅将 map 任务分配给本来就有所有输入文件的节点上,来减少一次网络调用使得性能得到提升
- 同时还可以使用一些流处理的思路优化 shuffle 的过程,那就是 当一个 map 任务完成后通知 main 进程后,main 进程立即通知 reduce 任务拉取其中一份文件,而不必等到所有 map 任务全部执行完毕后进行网络传输而提高了并行性

#### 6.2 任务的粒度

- 需考虑应该配置多少个任务执行 map，多少个任务执行 reduce
- 任务拆分过多加剧网络传输的负担。 而任务拆分的过少又会导致并行度不够而降低整体的执行效率
- 一些经验性的配置是 map 任务通常为输入文件总大小除以 64M 的值（ 这源于底层的分布式文件系统是以 64m 为一个 chuck 进行存储的 ），reduce 的数量通常是 map 任务的一半
- 同时为了发挥机器本身的多核特性，一台机器上可以指定多个 map 和 reduce 任务来执行，通常是任务总数的百分之一

#### 6.3 备用任务

- 如果最后的几个任务执行时间过长，比如存在 10 个任务用 5 分钟完成了其中 9 个，但最后一个任务因为当前机器的负载过高花费了 20 分钟执行完毕，这么整个任务的执行周期就是 20 分钟
- 我们可以当仅剩下 1% 的任务时，可以启动备用任务，即同时在两个节点上执行相同的任务。这样只要其中一个先返回即可结束整个任务，同时释放未完成的任务所占用的资源

#### 6.4 合并函数

- 如果 map 函数输出的同一种 key 的内容过多，比如在词频统计中，一些常见的谓词频率非常高。这就会产生非常多的类似 ( "are", 1 ) 这样的 kv 对，导致中间文件的大小严重不均衡，网络传输带宽加剧
- 解决这一问题的方案在于对传输的中间文件进行预处理，在 shuffle 之前对其进行一次 reduce 操作,将这种 ( "are", 1 ) 进行合并变为 ( "are", 1000000 )，减少需要传输数据的大小节省网络带宽
