## 1. 基础概念

#### 1.1 服务概念

- 命名空间
  - 用于进行租户粒度的配置隔离。不同的命名空间下，可以存在相同的 Group 或 Data ID 的配置
  - Namespace 的常用场景之一是不同环境的配置的区分隔离，例如开发测试环境和生产环境的资源（ 如配置、服务 ）隔离等
- 服务
  - 通过预定义接口网络访问的提供给客户端的软件功能
- 服务名
  - 服务提供的标识，通过该标识可以唯一确定其指代的服务
- 服务注册中心
  - 存储服务实例和服务负载均衡策略的数据库
- 服务发现
  - 在计算机网络上，（ 通常使用服务名 ）对服务下的实例的地址和元数据进行探测，并以预先定义的接口提供给客户端进行查询
- 元信息
  - Nacos 数据（ 如配置和服务 ）描述信息，如服务版本、权重、容灾策略、负载均衡策略、鉴权配置、各种自定义标签 (label )，从作用范围来看，分为服务级别的元信息、集群的元信息及实例的元信息
- 应用
  - 用于标识服务提供方的服务的属性
- 服务分组
  - 不同的服务可以归类到同一分组
- 虚拟集群
  - 同一个服务下的所有服务实例组成一个默认集群, 集群可以被进一步按需求划分，划分的单位可以是虚拟集群
- 实例
  - 提供一个或多个服务的具有可访问网络地址（ IP ）的进程
- 权重
  - 实例级别的配置。权重为浮点数。权重越大，分配给该实例的流量越大

#### 1.2 健康概念

- 健康检查
  - 以指定方式检查服务下挂载的实例（ Instance ）的健康度，从而确认该实例（ Instance ）是否能提供服务。根据检查结果，实例（ Instance ）会被判断为健康或不健康
  - 对服务发起解析请求时，不健康的实例（ Instance ）不会返回给客户端
- 健康保护阈值
  - 为了防止因过多实例（ Instance ）不健康导致流量全部流向健康实例（ Instance ），继而造成流量压力把健康实例（ Instance ）压垮并形成雪崩效应，应将健康保护阈值定义为一个 0 到 1 之间的浮点数
  - 当域名健康实例数（ Instance ）占总服务实例数（ Instance ）的比例小于该值时，无论实例（ Instance ）是否健康，都会将这个实例（ Instance ）返回给客户端
  - 这样做虽然损失了一部分流量，但是保证了集群中剩余健康实例（ Instance ）能正常工作

## 2. 数据模型

#### 2.1 演进背景

- 注册中心的核心数据是服务的名字和它对应的网络地址，当服务注册了多个实例时，我们需要对不健康的实例进行过滤或者针对实例的一些特征进行流量的分配，那么就需要在实例上存储一些例如健康状态、权重等属性
- 随着服务规模的扩大，渐渐的又需要在整个服务级别设定一些权限规则、以及对所有实例都生效的一些开关，于是在服务级别又会设立一些属性
- 再往后，我们又发现单个服务的实例又会有划分为多个子集的需求，例如一个服务是多机房部署的，那么可能需要对每个机房的实例做不同的配置，这样又需要在服务和实例之间再设定一个数据级别

#### 2.2 和其他框架比较

- Zookeeper 没有针对服务发现设计数据模型，它的数据是以一种更加抽象的树形 K-V 组织的，因此理论上可以存储任何语义的数据
- 而 Eureka 或者 Consul 都是做到了实例级别的数据扩展，这可以满足大部分的场景，不过无法满足大规模和多环境的服务数据存储
- Nacos 在经过内部多年生产经验后提炼出的数据模型，则是一种 **服务 - 集群 - 实例** 的三层模型。这样基本可以满足服务在所有场景下的数据存储和管理
  ![image](https://github.com/user-attachments/assets/e828adac-73fc-47bc-aff5-c229b357d445)
- Nacos 的数据模型虽然相对复杂，但是它并不强制你使用它里面的所有数据，在大多数场景下，你可以选择忽略这些数据属性，此时可以降维成和 Eureka 和 Consul 一样的数据模型

#### 2.3 数据隔离模型

- 作为一个共享服务型的组件，需要能够在多个用户或者业务方使用的情况下，保证数据的隔离和安全，这在稍微大一点的业务场景中非常常见。另一方面服务注册中心往往会支持云上部署，此时就要求服务注册中心的数据模型能够适配云上的通用模型
- Zookeeper、Consul 和 Eureka 在开源层面都没有很明确的针对服务隔离的模型，Nacos 则在一开始就考虑到如何让用户能够以多种维度进行数据隔离，同时能够平滑的迁移到阿里云上对应的商业化产品
- Nacos 隔离模型，提供了四层的数据逻辑隔离模型
  ![image](https://github.com/user-attachments/assets/5da74816-ff3d-4244-b2f2-7dd5baeea90d)
  - 用户账号对应的可能是一个企业或者独立的个体，这个数据一般情况下不会透传到服务注册中心
  - 一个用户账号可以新建多个命名空间，每个命名空间对应一个客户端实例，这个命名空间对应的注册中心物理集群是可以根据规则进行路由的，这样可以让注册中心内部的升级和迁移对用户是无感知的，同时可以根据用户的级别，为用户提供不同服务级别的物理集群
  - 再往下是服务分组和服务名组成的二维服务标识，可以满足接口级别的服务隔离

#### 2.4 临时实例和持久化实例（ Nacos 1.0 ）

- 临时实例和持久化实例
  ![image](https://github.com/user-attachments/assets/3e101e90-1c1a-4406-b50c-00d8eb54dab2)
- Nacos 1.0.0 在定义上区分临时实例和持久化实例的关键是健康检查的方式
- 临时实例使用客户端上报模式
  - 临时实例需要能够自动摘除不健康实例，而且无需持久化存储实例，那么这种实例就适用于类 Gossip 的协议
- 持久化实例使用服务端反向探测模式
  - 因为客户端不会上报心跳，那么自然就不能去自动摘除下线的实例
- 在大中型的公司里，这两种类型的服务往往都有
  - 一些基础的组件例如数据库、缓存等，这些往往不能上报心跳，这种类型的服务在注册时，就需要作为持久化实例注册
  - 而上层的业务服务，例如微服务或者 Dubbo 服务，服务的 Provider 端支持添加汇报心跳的逻辑，此时就可以使用动态服务的注册方式

#### 2.5 临时实例和持久化实例（ Nacos 2.0 ）

- Nacos 2.0 中继续沿用了持久化及非持久化的设定，但是有了一些调整
- Nacos 1.0 中持久化及非持久化的属性是作为实例的一个元数据进行存储和识别。这导致同一个服务下可以同时存在持久化实例和非持久化实例
- 但是在实际使用中，我们发现这种模式会给运维人员带来极大的困惑和运维复杂度；与此同时，从系统架构来看，一个服务同时存在持久化及非持久化实例的场景也是存在一定矛盾的。这就导致该能力事实上并未被广泛使用
- 为了简化 Nacos 的服务数据模型，降低运维人员的复杂度，提升 Nacos 的易用性，在 Nacos2.0 中我们将是否持久化的数据抽象至服务级别，且不再允许一个服务同时存在持久化实例和非持久化实例，实例的持久化属性继承自服务的持久化属性

## 3. 数据一致性

#### 3.1 一致性分类

- 从协议层面上看，一致性的选型已经很长时间没有新的成员加入了。目前来看基本可以归为两类
  - 一种是基于 Leader 的非对等部署的单点写一致性（ 单主模型 ）
  - 一种是对等部署的多写一致性（ 无主模型 ）
- 当我们选用服务注册中心的时候，并没有一种协议能够覆盖所有场景，例如当注册的服务节点不会定时发送心跳到注册中心时，强一致协议看起来是唯一的选择，因为无法通过心跳来进行数据的补偿注册，第一次注册就必须保证数据不会丢失
- 而当客户端会定时发送心跳来汇报健康状态时，第一次的注册的成功率并不是非常关键（ 当然也很关键，只是相对来说我们容忍数据的少量写失败 ），因为后续还可以通过心跳再把数据补偿上来，此时 Paxos 协议的单点瓶颈就会不太划算了，这也是 Eureka 为什么不采用 Paxos 协议而采用自定义的 Renew 机制的原因。

#### 3.2 Zk 存在的问题

- 这两种数据一致性协议有各自的使用场景，对服务注册的需求不同，就会导致使用不同的协议
- 在这里可以发现，Zookeeper 在 Dubbo 体系下表现出的行为，其实采用 Eureka 的 Renew 机制更加合适，因为 Dubbo 服务往 Zookeeper 注册的就是临时节点，需要定时发心跳到 Zookeeper 来续约节点，并允许服务下线时，将 Zookeeper 上相应的节点摘除
- Zookeeper 使用 ZAB 协议虽然保证了数据的强一致，但是它的机房容灾能力的缺乏，无法适应一些大型场景

#### 3.3 Nacos 实现

- Nacos 因为要支持多种服务类型的注册，并能够具有机房容灾、集群扩展等必不可少的能力，在 1.0.0 正式支持 AP 和 CP 两种一致性协议并存
- 1.0.0 重构了数据的读写和同步逻辑，将与业务相关的 CRUD 与底层的一致性同步逻辑进行了分层隔离
- 然后将业务的读写（ 主要是写，因为读会直接使用业务层的缓存 ）抽象为 Nacos 定义的数据类型，调用一致性服务进行数据同步。在决定使用 CP 还是 AP 一致性时，使用一个代理，通过可控制的规则进行转发
- 目前的一致性协议实现，一个是基于简化的 Raft 的 CP 一致性，一个是基于自研协议 Distro 的 AP 一致性
  ![image](https://github.com/user-attachments/assets/3462f2c8-231f-424c-9df4-f6a3568fe994)
  - Raft 协议不必多言，基于 Leader 进行写入，其 CP 也并不是严格的，只是能保证一半所见一致，以及数据的丢失概率较小
  - Distro 协议则是参考了内部 ConfigServer 和开源 Eureka ，在不借助第三方存储的情况下，实现基本大同小异。Distro 重点是做了一些逻辑的优化和性能的调优

## 4. 负载均衡

#### 4.1 其他框架

- 负载均衡严格的来说，并不算是传统注册中心的功能
- 一般来说服务发现的完整流程应该是先从注册中心获取到服务的实例列表，然后再根据自身的需求，来选择其中的部分实例或者按照一定的流量分配机制来访问不同的服务提供者，因此注册中心本身一般不限定服务消费者的访问策略
  ![image](https://github.com/user-attachments/assets/003c69ba-415e-4121-822f-aacb2d14640f)
- Eureka、Zookeeper 包括 Consul，本身都没有去实现可配置及可扩展的负载均衡机制，Eureka 的负载均衡是由 ribbon 来完成的，而 Consul 则是由 Fabio 做负载均衡

#### 4.2 Nacos 实现思路

- 在阿里巴巴集团内部，却是使用的相反的思路
- 服务消费者往往并不关心所访问的服务提供者的负载均衡，它们只关心以最高效和正确的访问服务提供者的服务。而服务提供者，则非常关注自身被访问的流量的调配，这其中的第一个原因是，阿里巴巴集团内部服务访问流量巨大，稍有不慎就会导致流量异常压垮服务提供者的服务。因此服务提供者需要能够完全掌控服务的流量调配，并可以动态调整
- 服务端的负载均衡，给服务提供者更强的流量控制权，但是无法满足不同的消费者希望使用不同负载均衡策略的需求。而不同负载均衡策略的场景，确实是存在的。而客户端的负载均衡则提供了这种灵活性，并对用户扩展提供更加友好的支持。但是客户端负载均衡策略如果配置不当，可能会导致服务提供者出现热点，或者压根就拿不到任何服务提供者
  ![image](https://github.com/user-attachments/assets/b40735cc-f85c-4937-8b2c-579a316d7cb8)
- 理想的负载均衡实现应该是什么样的呢？不同的人会有不同的答案。Nacos 试图做的是将服务端负载均衡与客户端负载均衡通过某种机制结合起来，提供用户扩展性，并给予用户充分的自主选择权和轻便的使用方式
- 负载均衡是一个很大的话题，当我们在关注注册中心提供的负载均衡策略时，需要注意该注册中心是否有我需要的负载均衡方式，使用方式是否复杂。如果没有，那么是否允许我方便的扩展来实现我需求的负载均衡策略

## 5. 健康检查

#### 5.1 客户端上报模式

- Zookeeper 和 Eureka 都实现了一种 TTL 的机制，就是如果客户端在一定时间内没有向注册中心发送心跳，则会将这个客户端摘除。Eureka 做的更好的一点在于它允许在注册服务的时候，自定义检查自身状态的健康检查方法
- 这在服务实例能够保持心跳上报的场景下，是一种比较好的体验，在 Dubbo 和 SpringCloud 这两大体系内，也被培养成用户心智上的默认行为
- Nacos 也支持这种 TTL 机制，不过这与 ConfigServer 在阿里巴巴内部的机制又有一些区别。Nacos 目前支持临时实例使用心跳上报方式维持活性，发送心跳的周期默认是 5 秒，Nacos 服务端会在 15 秒没收到心跳后将实例设置为不健康，在 30 秒没收到心跳时将这个临时实例摘除

#### 5.2 服务端探测模式

- 不过正如前文所说，有一些服务无法上报心跳，但是可以提供一个检测接口，由外部去探测。这样的服务也是广泛存在的，而且以我们的经验，这些服务对服务发现和负载均衡的需求同样强烈
- 服务端健康检查最常见的方式是 TCP 端口探测和 HTTP 接口返回码探测，这两种探测方式因为其协议的通用性可以支持绝大多数的健康检查场景
- 在其他一些特殊的场景中，可能还需要执行特殊的接口才能判断服务是否可用。例如部署了数据库的主备，数据库的主备可能会在某些情况下切换，需要通过服务名对外提供访问，保证当前访问的库是主库。此时的健康检查接口，可能就是一个检查数据库是否是主库的 MYSQL 命令了

#### 5.3 两者区别

- 客户端健康检查和服务端健康检查有一些不同的关注点
  - 客户端健康检查主要关注客户端上报心跳的方式、服务端摘除不健康客户端的机制
  - 而服务端健康检查，则关注探测客户端的方式、灵敏度及设置客户端健康状态的机制
- 从实现复杂性来说，服务端探测肯定是要更加复杂的
  - 因为需要服务端根据注册服务配置的健康检查方式，去执行相应的接口，判断相应的返回结果，并做好重试机制和线程池的管理。这与客户端探测，只需要等待心跳，然后刷新 TTL 是不一样的
  - 同时服务端健康检查无法摘除不健康实例，这意味着只要注册过的服务实例，如果不调用接口主动注销，这些服务实例都需要去维持健康检查的探测任务，而客户端则可以随时摘除不健康实例，减轻服务端的压力。

#### 5.4 Nacos 实现思路

- Nacos 的健康检查
  ![image](https://github.com/user-attachments/assets/5c3c2ec3-29f0-44ee-a418-531b612d4089)
- Nacos 既支持客户端的健康检查，也支持服务端的健康检查，同一个服务可以切换健康检查模式
- 我们认为这种健康检查方式的多样性非常重要，这样可以支持各种类型的服务，让这些服务都可以使用到 Nacos 的负载均衡能力
- Nacos 下一步要做的是实现健康检查方式的用户扩展机制，不管是服务端探测还是客户端探测。这样可以支持用户传入一条业务语义的请求，然后由 Nacos 去执行，做到健康检查的定制
