## 1. ZooKeeper 作为注册中心

#### 1.1 RPC 服务注册 / 发现过程简述

1. 服务提供者启动时，会将其服务名称，IP 地址注册到配置中心
2. 服务消费者在第一次调用服务时，会通过注册中心找到相应的服务的 IP 地址列表，并缓存到本地，以供后续使用。当消费者调用服务时，不会再去请求注册中心，而是直接通过负载均衡算法从 IP 列表中取一个服务提供者的服务器调用服务
3. 当服务提供者的某台服务器宕机或下线时，相应的 IP 会从服务提供者 IP 列表中移除。同时，注册中心会将新的服务 IP 地址列表发送给服务消费者机器，缓存在消费者本机
4. 当某个服务的所有服务器都下线了，那么这个服务也就下线了
5. 同样，当服务提供者的某台服务器上线时，注册中心会将新的服务 IP 地址列表发送给服务消费者机器，缓存在消费者本机
6. 服务提供方可以根据服务消费者的数量来作为服务下线的依据

#### 1.2 ZooKeeper 注册中心原理

- ZooKeeper 可以充当一个服务注册表（ Service Registry ），让多个服务提供者形成一个集群，让服务消费者通过服务注册表获取具体的服务访问地址（ IP + 端口 ）去访问具体的服务提供者
  ![image](https://img-blog.csdnimg.cn/e13b9dce4b154920a3568f24849f84ef.png)
- 分布式应用中，通常需要有一套完整的命名规则，既能够产生唯一的名称又便于人识别和记住，通常情况下用树形的名称结构是一个理想的选择，树形的名称结构是一个有层次的目录结构。通过调用 ZooKeeper 提供的创建节点的 API，能够很容易创建一个全局唯一的 path，这个 path 就可以作为一个名称
- 在 ZooKeeper 中，进行服务注册，实际上就是在 ZooKeeper 中创建了一个 ZNode 节点，该节点存储了该服务的 IP、端口、调用方式（ 协议、序列化方式 ）等。该节点承担着最重要的职责，它由服务提供者（ 发布服务时 ）创建，以供服务消费者获取节点中的信息，从而定位到服务提供者真正网络拓扑位置以及得知如何调用

#### 1.3 ZooKeeper 注册流程

- 每当一个服务提供者部署后都要将自己的服务注册到 ZooKeeper 的某一路径上：/{service}/{version}/{ip:port}
- 比如我们的 HelloWorldService 部署到两台机器，那么 ZooKeeper 上就会创建两条目录
- 例如
  ![image](https://img-blog.csdnimg.cn/0d909b3d6f104fb8a44fe18d29e4ed8d.png)

#### 1.4 ZooKeeper 的 心跳检测

- ZooKeeper 的心跳检测，可以自动探测服务提供者机器的宕机或下线（ 对应 第 3 步中 “当服务提供者的某台服务器宕机或下线时” ）
- ZooKeeper 提供了 心跳检测 功能，它会定时向各个服务提供者发送一个请求（ 实际上建立的是一个 socket 长连接 ），如果长期没有响应，服务中心就认为该服务提供者已经 “挂了”，并将其剔除。

#### 1.5 ZooKeeper 的 Watch 机制

- ZooKeeper 的 Watch 机制，可以将变更的注册列表推给服务消费者 （ 对应 第 3 步和第 5 步中 “注册中心会将新的服务 IP 地址列表发送给服务消费者机器” ）
- 解决的方式有两种
  - 主动拉取策略：服务的消费者定期调用注册中心提供的服务获取接口获取最新的服务列表并更新本地缓存，经典案例就是 Eureka
    ![image](https://img-blog.csdnimg.cn/290510828020444fbe7b123aac519254.png)
  - 发布-订阅模式：服务消费者能够实时监控服务更新状态，通常采用监听器以及回调机制
    ![image](https://img-blog.csdnimg.cn/a44f320bf4e3446996e7654b1c543b58.png)
- ZooKeeper 使用的是“发布-订阅模式”，这里就要提到 ZooKeeper 的 Watch 机制，整体流程如下：
  - 客户端先向 ZooKeeper 服务端成功注册想要监听的节点状态，同时客户端本地会存储该监听器相关的信息在 WatchManager 中
  - 当 ZooKeeper 服务端监听的数据状态发生变化时，ZooKeeper 就会主动通知发送相应事件信息给相关会话客户端，客户端就会在本地响应式的回调相关 Watcher 的 Handler
- 即 ZooKeeper 的 Watch 机制其实就是一种推拉结合的模式：
  - 服务消费者会去监听相应路径（ /HelloWorldService/1.0.0 ），一旦路径上的数据有任务变化（ 增加或减少 ），ZooKeeper 只会发送一个事件类型和节点信息给关注的客户端，而不会包括具体的变更内容，所以事件本身是轻量级的，这就是推的部分
  - 收到变更通知的客户端需要自己去拉变更的数据，这就是拉的部分

#### 1.6 ZooKeeper 是否适合作为注册中心

- 作为一个分布式协同服务，ZooKeeper 非常好，但是对于 Service 发现服务来说就不合适了
- 因为对于 Service 发现服务来说就算是返回了包含不实的信息的结果也比什么都不返回要好。所以当向注册中心查询服务列表时，我们可以容忍注册中心返回的是几分钟以前的注册信息，但不能接受服务直接 down 掉不可用
- 但是 ZooKeeper 会出现这样一种情况，当 master 节点因为网络故障与其他节点失去联系时，剩余节点会重新进行 leader 选举
- 问题在于，选举 leader 的时间太长，30 ~ 120s, 且选举期间整个 ZooKeeper 集群都是不可用的，这就导致在选举期间注册服务瘫痪
- 在云部署的环境下，因网络问题使得 ZooKeeper 集群失去 master 节点是较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的
- 所以说，作为注册中心，可用性的要求要高于一致性
- 在 CAP 模型中，ZooKeeper 整体遵循一致性（ CP ）原则，即在任何时候对 ZooKeeper 的访问请求能得到一致的数据结果，但是当机器下线或者宕机时，不能保证服务可用性。
- 那为什么 ZooKeeper 不使用最终一致性（ AP ）模型呢？因为这个依赖 ZooKeeper 的核心算法是 ZAB，所有设计都是为了强一致性。这个对于分布式协调系统，完全没没有毛病，但是你如果将 ZooKeeper 为分布式协调服务所做的一致性保障，用在注册中心，或者说服务发现场景，这个其实就不合适

## 2. 配置中心

- 通常通过 数据的发布 / 订阅系统
- 在分布式系统中，可能有成千上万个服务节点，如果想要对所有服务的某项配置进行更改，由于数据节点过多，不可逐台进行修改，而应该在设计时采用统一的配置中心
- 之后发布者只需要将新的配置发送到配置中心，所有服务节点即可自动下载并进行更新，从而实现配置的集中管理和动态更新
- ZooKeeper 通过 Watcher 机制可以实现数据的发布和订阅
- 分布式系统的所有的服务节点可以对某个 ZNode 注册监听，之后只需要将新的配置写入该 ZNode，所有服务节点都会收到该事件

## 3. 选举 Leader 节点

- 分布式系统一个重要的模式就是主从模式 （ Master / Salves ），ZooKeeper 可以用于该模式下的 Matser 选举。可以让所有服务节点去竞争性地创建同一个 ZNode，由于 ZooKeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，这样该服务节点就可以成为 Master 节点

## 4. 分布式锁

#### 4.1 操作说明

- 可以通过 ZooKeeper 的临时节点和 Watcher 机制来实现分布式锁
- 以排它锁为例进行说明：
  - 分布式系统的所有服务节点可以竞争性地去创建同一个临时 ZNode，由于 ZooKeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，此时可以认为该节点获得了锁。其他没有获得锁的服务节点通过在该 ZNode 上注册监听，从而当锁释放时再去竞争获得锁。锁的释放情况有以下两种：
    - 当正常执行完业务逻辑后，客户端主动将临时 ZNode 删除，此时锁被释放
    - 当获得锁的客户端发生宕机时，临时 ZNode 会被自动删除，此时认为锁已经释放
  - 当锁被释放后，其他服务节点则再次去竞争性地进行创建，但每次都只有一个服务节点能够获取到锁，这就是排他锁

#### 4.2 排它锁举例

- 举例来说，有一个分布式系统，有三个节点 A、B、C，试图通过 ZooKeeper 获取分布式锁
  - 访问 /lock （ 这个目录路径由程序自己决定 ），创建带序列号的临时节点（ EPHEMERAL ）
    ![image](https://github.com/user-attachments/assets/84504143-dcc1-4916-8a68-12c65ea07fa2)
  - 每个节点尝试获取锁时，拿到 /locks 节点下的所有子节点（id_0000,id_0001,id_0002），判断自己创建的节点是不是最小的
    ![image](https://github.com/user-attachments/assets/13defaf7-ad5c-4da8-8e9a-dcff4feffe44)
    - 如果是，则拿到锁。释放锁时（ 即执行完操作后 ），把创建的节点给删掉
    - 如果不是，则监听比自己要小 1 的节点变化
  - 释放锁，即删除自己创建的节点
    ![image](https://github.com/user-attachments/assets/1803521f-996a-4146-9dbc-7c72475bf573)
    - NodeA 删除自己创建的节点 id_0000，NodeB 监听到变化，发现自己的节点已经是最小节点，即可获取到锁

## 5. 集群管理

- ZooKeeper 还能解决大多数分布式系统中的问题：
  - 如可以通过创建临时节点来建立心跳检测机制。如果分布式系统的某个服务节点宕机了，则其持有的会话会超时，此时该临时节点会被删除，相应的监听事件就会被触发
  - 分布式系统的每个服务节点还可以将自己的节点状态写入临时节点，从而完成状态报告或节点工作进度汇报
  - 通过数据的订阅和发布功能，ZooKeeper 还能对分布式系统进行模块的解耦和任务的调度
  - 通过监听机制，还能对分布式系统的服务节点进行动态上下线，从而实现服务的动态扩容

## 6. 队列管理

- ZooKeeper 可以处理两种类型的队列：
  - 当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达，这种是同步队列
  - 队列按照 FIFO 方式进行入队和出队操作，例如实现生产者和消费者模型
- 同步队列用 ZooKeeper 实现的实现思路如下：
  - 创建一个父目录 /synchronizing，每个成员都监控标志（ Set Watch ）位目录 /synchronizing/start 是否存在，然后每个成员都加入这个队列
  - 加入队列的方式就是创建 /synchronizing/member_i 的临时目录节点，然后每个成员获取 / synchronizing 目录的所有目录节点，也就是 member_i
  - 判断 i 的值是否已经是成员的个数，如果小于成员个数等待 /synchronizing/start 的出现，如果已经相等就创建 /synchronizing/start

## 7. 常用 ZooKeeper 实现包

- [node-zookeeper](https://github.com/yfinkelstein/node-zookeeper)
  - Apache ZooKeeper 的 Node.js 客户端，是在官方 ZooKeeper C Client API 之上实现的，支持 ZooKeeper Server 版本 v3.5.x - v3.8.x
  - 底层源码是通过在 src 目录下，C 语言调用 ZooKeeper C Client API，最后通过 V8 引擎的原生绑定，转换成 prebuilds 下的 .node 文件，最后在 js 文件中，引入 .node 文件来使用
