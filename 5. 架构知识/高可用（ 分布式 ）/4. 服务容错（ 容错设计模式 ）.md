## 1. 容错设计模式

- **这里容错设计模式指的是 “要实现某种容错策略，我们该如何去做”**
- 为了实现各种各样的容错策略，开发人员总结出了一些被实践证明是有效的服务容错设计模式，譬如微服务中常见的断路器模式、舱壁隔离模式，重试模式，等等
- 三种容错模式分别对应的策略为
  - 使用 断路器模式 实现 **快速失败** 策略
  - 使用 舱壁隔离模式 实现 **静默失败** 策略
  - 使用 重试模式 实现 **故障转移和故障恢复** 策略

## 2. 断路器模式（ 也就是服务熔断 ）

- 断路器模式是微服务架构中最基础的容错设计模式，以至于像 Hystrix 这种服务治理工具往往被人们忽略了它的服务隔离、请求合并、请求缓存等其他服务治理职能，直接将它称之为微服务 断路器 或者 熔断器
- 断路器即在互联网系统中，当下游服务因访问压力过大而响应变慢或失败，上游服务为了保护系统整体的可用性，可以暂时切断对下游服务的调用
- 这个设计模式最早由技术作家 Michael Nygard 在《Release It!》一书中提出的，后又因 Martin Fowler 的《Circuit Breaker》一文而广为人知

#### 2.1 基本思路

- 断路器的基本思路是很简单的，就是通过代理（ 断路器对象 ）来一对一地（ 一个远程服务对应一个断路器对象 ）**接管服务调用者的远程请求**
- 断路器会持续监控并统计服务返回的成功、失败、超时、拒绝等各种结果，当出现故障（ 失败、超时、拒绝 ）的次数达到断路器的阈值时，它状态就自动变为 “OPEN”，后续此断路器代理的远程访问都将直接返回调用失败，而不会发出真正的远程服务请求
- 通过断路器对远程服务的熔断，避免因持续的失败或拒绝而消耗资源，因持续的超时而堆积请求，最终的目的就是避免雪崩效应的出现
- 由此可见，断路器本质是一种快速失败策略的实现方式，它的工作过程可以通过下图来表示
  ![image](https://github.com/user-attachments/assets/4e74dc61-8b8b-493a-ba3b-da36ead02137)

#### 2.2 断路器状态

- 从调用序列来看，断路器就是一种有限状态机，断路器模式就是根据自身状态变化自动调整代理请求策略的过程。一般要设置以下三种断路器的状态：CLOSE，OPEN，HALF OPEN
- CLOSED：
  - 表示断路器关闭，此时的远程请求会真正发送给服务提供者
  - 断路器刚刚建立时默认处于这种状态，此后将持续监视远程请求的数量和执行结果，决定是否要进入 OPEN 状态
- OPEN：
  - 表示断路器开启，此时不会进行远程请求，直接给服务调用者返回调用失败的信息，以实现快速失败策略
- HALF OPEN：
  - 这是一种中间状态
  - 断路器必须带有自动的故障恢复能力，当进入 OPEN 状态一段时间以后，将 “自动”（ 一般是由下一次请求而不是计时器触发的，所以这里自动带引号 ）切换到 HALF OPEN 状态
  - 该状态下，会放行一次远程调用，然后根据这次调用的结果成功与否，转换为 CLOSED 或者 OPEN 状态，以实现断路器的 **弹性恢复**

#### 2.3 状态转换

- 状态的转换逻辑与条件
  ![image](https://github.com/user-attachments/assets/438155e6-ede1-41ad-af94-2c23decd68ac)
- 重点是 OPEN 和 CLOSED 状态的转换条件是什么
- 最简单直接的方案是只要遇到一次调用失败，那就默认以后所有的调用都会接着失败，断路器直接进入 OPEN 状态，但这样做的效果是很差的，虽然避免了故障扩散和请求堆积，却使得外部看来系统将表现极其不稳定
- 现实中，比较可行的办法是在以下两个条件同时满足时，断路器状态转变为 OPEN：
  - **一段时间（ 譬如 10 秒以内 ）内请求数量达到一定阈值（ 譬如 20 个请求 ）**。这个条件的意思是如果请求本身就很少，那就用不着断路器介入
  - **一段时间（ 譬如 10 秒以内 ）内请求的故障率（ 发生失败、超时、拒绝的统计比例 ）到达一定阈值（ 譬如 50% ）**。这个条件的意思是如果请求本身都能正确返回，也用不着断路器介入
  - 以上两个条件同时满足时，断路器就会转变为 OPEN 状态。括号中举例的数值是 Netflix Hystrix 的默认值，其他服务治理的工具，譬如 Resilience4j、Envoy 等也同样会包含有类似的设置

#### 2.4 服务熔断中需考虑的设计

- 异常处理：
  - 调用受熔断器保护的服务的时候，我们必须要处理当服务不可用时的异常情况。这些异常处理通常需要视具体的业务情况而定。比如，如果应用程序只是暂时的功能降级，可能需要切换到其它的可替换的服务上来执行相同的任务或者获取相同的数据，或者给用户报告错误然后提示他们稍后重试
- 异常的类型：
  - 请求失败的原因可能有很多种。一些原因可能会比其它原因更严重。比如，请求会失败可能是由于远程的服务崩溃，这可能需要花费数分钟来恢复；也可能是由于服务器暂时负载过重导致超时
  - 熔断器应该能够检查错误的类型，从而根据具体的错误情况来调整策略。比如，可能需要很多次超时异常才可以断定需要切换到断开状态，而只需要几次错误提示就可以判断服务不可用而快速切换到断开状态
- 日志：
  - 熔断器应该能够记录所有失败的请求，以及一些可能会尝试成功的请求，使得的管理员能够监控使用熔断器保护的服务的执行情况
- 测试服务是否可用：
  - 在断开状态下，熔断器可以采用定期的 ping 远程的服务或者资源，来判断是否服务是否恢复，而不是使用计时器来自动切换到半断开状态。这种 ping 操作可以模拟之前那些失败的请求，或者可以使用通过调用远程服务提供的检查服务是否可用的方法来判断
- 手动重置：
  - 在系统中对于失败操作的恢复时间是很难确定的，提供一个手动重置功能能够使得管理员可以手动的强制将熔断器切换到闭合状态。同样的，如果受熔断器保护的服务暂时不可用的话，管理员能够强制的将熔断器设置为断开状态
- 并发问题：
  - 相同的熔断器有可能被大量并发请求同时访问。熔断器的实现不应该阻塞并发的请求或者增加每次请求调用的负担
- 资源的差异性：
  - 使用单个熔断器时，一个资源如果有分布在多个地方就需要小心
  - 比如，一个数据可能存储在多个磁盘分区上（ shard ），某个分区可以正常访问，而另一个可能存在暂时性的问题。在这种情况下，不同的错误响应如果混为一谈，那么应用程序访问的这些存在问题的分区的失败的可能性就会高，而那些被认为是正常的分区，就有可能被阻塞
- 加快熔断器的熔断操作：
  - 有时候，服务返回的错误信息足够让熔断器立即执行熔断操作并且保持一段时间
  - 比如，如果从一个分布式资源返回的响应提示负载超重，那么应该等待几分钟后再重试（ HTTP 协议定义了 ”HTTP 503 Service Unavailable” 来表示请求的服务当前不可用，他可以包含其他信息比如，超时等 ）
- 重复失败请求：
  - 当熔断器在断开状态的时候，熔断器可以记录每一次请求的细节，而不是仅仅返回失败信息，这样当远程服务恢复的时候，可以将这些失败的请求再重新请求一次
- 服务熔断恢复：
  - 如果服务是幂等性的，则恢复重试不会有问题；而如果服务是非幂等性的，则重试会导致数据出现问题

## 3. 舱壁隔离模式

- **舱壁隔离模式是常用的实现服务隔离的设计模式**，舱壁这个词是来自造船业的舶来品，它原本的意思是设计舰船时，要在每个区域设计独立的水密舱室，一旦某个舱室进水，也只是影响这个舱室中的货物，而不至于让整艘舰艇沉没。这种思想就很符合容错策略中失败静默策略

#### 3.1 使用场景

- 调用外部服务的故障大致可以分为三大类
  - “失败”：如 400 Bad Request、500 Internal Server Error 等错误
  - “拒绝”：如 401 Unauthorized、403 Forbidden 等错误
  - “超时”：如 408 Request Timeout、504 Gateway Timeout 等错误
- **其中 “超时” 引起的故障尤其容易给调用者带来全局性的风险**。这是由于目前主流的网络访问大多是基于 TPR 并发模型 来实现的，只要请求一直不结束（ 无论是以成功结束还是以失败结束 ），就要一直占用着某个线程不能释放
- 而线程是典型的整个系统的全局性资源，尤其是 Java 这类将线程映射为操作系统内核线程来实现的语言环境中，为了不让某一个远程服务的局部失败演变成全局性的影响，就必须设置某种止损方案，这便是服务隔离的意义

#### 3.2 场景举例

- 举例如下图
  ![image](https://github.com/user-attachments/assets/1c701eee-6b4d-466b-b869-c0ed9aac1bbf)
- 当分布式系统所依赖的某个服务，譬如下图中的 “服务 I” 发生了超时，那在高流量的访问下，假设平均 1 秒钟内对该服务的调用会发生 50 次，这就意味着该服务如果长时间不结束的话，每秒会有 50 条用户线程被阻塞
- 如果这样的访问量一直持续，我们按 Tomcat 默认的 HTTP 超时时间 20 秒来计算，20 秒内将会阻塞掉 1000 条用户线程，此后才陆续会有用户线程因超时被释放出来，回归 Tomcat 的全局线程池中
- 一般 Java 应用的线程池最大只会设置到 200 至 400 之间，这意味着此时系统在外部将表现为所有服务的全面瘫痪，而不仅仅是只有涉及到 “服务 I” 的功能不可用，因为 Tomcat 已经没有任何空余的线程来为其他请求提供服务了

#### 3.3 局部线程池

- 对于上面这类情况，一种可行的解决办法是 **为每个服务单独设立线程池**，这些线程池默认不预置活动线程，只用来控制单个服务的最大连接数
- 譬如，对出问题的 “服务 I” 设置了一个最大线程数为 5 的线程池，这时候它的超时故障就只会最多阻塞 5 条用户线程，而不至于影响全局
- 此时，其他不依赖 “服务 I” 的用户线程依然能够正常对外提供服务，如下图所示
  ![image](https://github.com/user-attachments/assets/da8a9f1d-4df5-4b43-bf92-13421a31b616)

#### 3.4 优缺点

- 使用局部的线程池来控制服务的最大连接数有许多好处，当服务出问题时能够隔离影响，当服务恢复后，还可以通过清理掉局部线程池，瞬间恢复该服务的调用，而如果是 Tomcat 的全局线程池被占满，再恢复就会十分麻烦
- 但是，局部线程池有一个显著的弱点，它额外增加了 CPU 的开销，每个独立的线程池都要进行排队、调度和下文切换工作
- 根据 Netflix 官方给出的数据，一旦启用 Hystrix 线程池来进行服务隔离，大概会为每次服务调用增加约 3 毫秒至 10 毫秒的延时，如果调用链中有 20 次远程服务调用，那每次请求就要多付出 60 毫秒至 200 毫秒的代价来换取服务隔离的安全保障

#### 3.5 信号量机制（ Semaphore ）

- 为应对上述缺点情况，还有一种更轻量的可以用来控制服务最大连接数的办法：信号量机制
- 如果不考虑清理线程池、客户端主动中断线程这些额外的功能，仅仅是为了控制一个服务并发调用的最大次数，可以只 **为每个远程服务维护一个线程安全的计数器** 即可，并不需要建立局部线程池
- 具体做法是当服务开始调用时计数器加 1，服务返回结果后计数器减 1，一旦计数器超过设置的阈值就立即开始限流，在回落到阈值范围之前都不再允许请求了
- 由于不需要承担线程的排队、调度、切换工作，所以单纯维护一个作为计数器的信号量的性能损耗，相对于局部线程池来说几乎可以忽略不计

#### 3.6 其他场景

- 上面介绍的是从微观的、服务调用的角度应用的舱壁隔离设计模式，舱壁隔离模式还可以在更高层、更宏观的场景中使用，不是按调用线程，而是按功能、按子系统、按用户类型等条件来隔离资源都是可以的
- 譬如，根据用户等级、用户是否 VIP、用户来访的地域等各种因素，将请求分流到独立的服务实例去，这样即使某一个实例完全崩溃了，也只是影响到其中某一部分的用户，把波及范围尽可能控制住
- 一般来说，我们会选择将服务层面的隔离实现在服务调用端或者边车代理上，将系统层面的隔离实现在 DNS 或者网关处

## 4. 重试模式

#### 4.1 基础介绍

- 故障转移和故障恢复策略都需要对服务进行重复调用，差别是这些重复调用有可能是同步的，也可能是后台异步进行；有可能会重复调用同一个服务，也可能会调用到服务的其他副本。无论具体是通过怎样的方式调用、调用的服务实例是否相同，都可以归结为重试设计模式的应用范畴
- 重试模式适合解决系统中的瞬时故障，简单的说就是有可能自己恢复（ Resilient，称为自愈，也叫做回弹性 ）的临时性失灵，网络抖动、服务的临时过载（ 典型的如返回了 503 Bad Gateway 错误 ）这些都属于瞬时故障

#### 4.2 重试的前提条件

- 重试模式实现并不困难，即使完全不考虑框架的支持，靠程序员自己编写十几行代码也能够完成。在实践中，重试模式面临的风险反而大多来源于太过简单而导致的滥用。我们判断是否应该且是否能够对一个服务进行重试时，应同时满足以下几个前提条件：
  - **仅在主路逻辑的关键服务上进行同步的重试**，不是关键的服务，一般不把重试作为首选容错方案，尤其不该进行同步重试
  - **仅对由瞬时故障导致的失败进行重试**。尽管一个故障是否属于可自愈的瞬时故障并不容易精确判定，但从 HTTP 的状态码上至少可以获得一些初步的结论，譬如，当发出的请求收到了 401 Unauthorized 响应，说明服务本身是可用的，只是你没有权限调用，这时候再去重试就没有什么意义。功能完善的服务治理工具会提供具体的重试策略配置（如 Envoy 的 Retry Policy），可以根据包括 HTTP 响应码在内的各种具体条件来设置不同的重试参数
  - **仅对具备幂等性的服务进行重试**。如果服务调用者和提供者不属于同一个团队，那服务是否幂等其实也是一个难以精确判断的问题，但仍可以找到一些总体上通用的原则。譬如，RESTful 服务中的 POST 请求是非幂等的，而 GET、HEAD、OPTIONS、TRACE 由于不会改变资源状态，这些请求应该被设计成幂等的；PUT 请求一般也是幂等的，因为 n 个 PUT 请求会覆盖相同的资源 n-1 次；DELETE 也可看作是幂等的，同一个资源首次删除会得到 200 OK 响应，此后应该得到 204 No Content 响应。这些都是 HTTP 协议中定义的通用的指导原则，虽然对于具体服务如何实现并无强制约束力，但我们自己建设系统时，遵循业界惯例本身就是一种良好的习惯

#### 4.3 重试的终止条件

- 重试必须有明确的终止条件，常用的终止条件有两种：
  - **超时终止**：并不限于重试，所有调用远程服务都应该要有超时机制避免无限期的等待。这里只是强调重试模式更加应该配合上超时机制来使用，否则重试对系统很可能反而是有害的，笔者已经在前面介绍故障转移策略时举过具体的例子，这里就不重复了。
  - **次数终止**：重试必须要有一定限度，不能无限制地做下去，通常最多就只重试 2 至 5 次。重试不仅会给调用者带来负担，对于服务提供者也是同样是负担。所以应避免将重试次数设的太大。此外，如果服务提供者返回的响应头中带有 Retry-After 的话，尽管它没有强制约束力，我们也应该充分尊重服务端的要求，做个 “有礼貌” 的调用者

#### 4.4 重试的弊端

- 由于重试模式可以在网络链路的多个环节中去实现，譬如客户端发起调用时自动重试，网关中自动重试、负载均衡器中自动重试，等等，而且现在的微服务框架都足够便捷，只需设置一两个开关参数就可以开启对某个服务甚至全部服务的重试机制。所以，对于没有太多经验的程序员，有可能根本意识不到其中会带来多大的负担
- 举个具体例子：一套基于 Netflix OSS 建设的微服务系统，如果同时在 Zuul、Feign 和 Ribbon 上都打开了重试功能，且不考虑重试被超时终止的话，那总重试次数就相当于它们的重试次数的乘积。假设按它们都重试 4 次，且 Ribbon 可以转移 4 个服务副本来计算，理论上最多会产生高达 4 × 4 × 4 × 4 = 256 次调用请求
